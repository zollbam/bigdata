{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  알고리즘 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv4 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈로딩\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일에 저장되어 있는 클래스 종류\n",
      " ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      " 종류의 개수=80\n"
     ]
    }
   ],
   "source": [
    "# YOLO로드\n",
    "net=cv2.dnn.readNet('yolov3.weights','yolov3.cfg') # 네트워크 불러오기\n",
    "classes=[]\n",
    "\n",
    "with open('coco.names','r') as f: # coco.names의 파일 안에는 다양한 종류의 클래스가 있음\n",
    "    classes=[line.strip() for line in f.readlines()]\n",
    "\n",
    "## 클래스 확인\n",
    "print(f'파일에 저장되어 있는 클래스 종류\\n {classes}\\n 종류의 개수={len(classes)}')\n",
    "\n",
    "layer_names=net.getLayerNames() # 네트워크의 모든 레이어 이름을 가지고 옴 => conv_0, conv_26 등등\n",
    "output_layers=[layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "## 1. net.getUnconnectedOutLayers() : 출력레이어 가져오기\n",
    "## 2. i-1부분은 출처에는 i[0]-1라고 되어 있는데 오류가 나오는데 왜 [0]을 해줘야 할까??\n",
    "## 3. YOLO에눈 3개의 출력 레이어가 있음 => 82/94/106\n",
    "\n",
    "colors=np.random.uniform(0,255,size=(len(classes),3))\n",
    "## 1. 80행 3열로 된 0~255인 균일분포의 값을 랜덤 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 불러오기\n",
    "img=cv2.imread(r'C:\\Users\\USER\\Desktop\\com_project\\data\\rawdata\\C\\[emergency]accident12_055C\\[emergency]accident12_055C_840.jpg')\n",
    "## 1. 이미지를 불러와 배열 형태로 저장 => 배열구조는 (1080,1920,3)\n",
    "# img=cv2.resize(img,(516,516),fx=0.4,fy=0.4)\n",
    "# ## 1. 불러온 이미지 사이즈 변경 => 배열구조는 (516,516,3)\n",
    "height,width,channels=img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 탐지\n",
    "blob = cv2.dnn.blobFromImage(img, 0.00395, (608, 608), (0, 0, 0), True, crop=False)\n",
    "## 1. 크기는 320/416/608 3개가 있음\n",
    "## 2. blobFromImage로 이미지 전처리하고 blob에 전달\n",
    "## 3. blob형식은 Mat타입의 4차원(4D Tensor:NCHW) 행렬 => N(영상개수) C(채널 개수) H(영상 세로) W(영상 가로)\n",
    "net.setInput(blob) # 생성된 blob객체로 네트워크 입력으로 설정\n",
    "outs = net.forward(output_layers) # 네트워크를 순방향으로 실행하여 결과를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정보 저장???\n",
    "class_ids = [] # class종류 인덱스\n",
    "confidences = [] # 신뢰도\n",
    "boxes = [] # 바운딩 박스의 4개 좌표\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores) # 최대값 인덱스 추출\n",
    "        confidence = scores[class_id] # 신뢰도\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            # 좌표\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            \n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[910, 240, 303, 596], [968, 240, 366, 707], [948, 488, 176, 299]]\n"
     ]
    }
   ],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[np.argmax(outs[0][0][5:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노이즈 제거\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 화면에 표시\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(img, label, (x, y + 30), font, 3, color, 3)\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 찾기 함수\n",
    "def dect_person(path):\n",
    "    # 이미지 불러오기\n",
    "    img=cv2.imread(path)\n",
    "    ## 1. 이미지를 불러와 배열 형태로 저장 => 배열구조는 (1080,1920,3)\n",
    "    # img=cv2.resize(img,(516,516),fx=0.4,fy=0.4)\n",
    "    # ## 1. 불러온 이미지 사이즈 변경 => 배열구조는 (516,516,3)\n",
    "    height,width,channels=img.shape\n",
    "\n",
    "    # 객체 탐지\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00395, (608, 608), (0, 0, 0), True, crop=False)\n",
    "    ## 1. 크기는 320/416/608 3개가 있음\n",
    "    ## 2. blobFromImage로 이미지 전처리하고 blob에 전달\n",
    "    ## 3. blob형식은 Mat타입의 4차원(4D Tensor:NCHW) 행렬 => N(영상개수) C(채널 개수) H(영상 세로) W(영상 가로)\n",
    "    print(blob.shape)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # 정보 저장???\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.4:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # 좌표\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    print(boxes)\n",
    "    \n",
    "    # 노이즈 제거\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # 화면에 표시\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label, (x, y + 30), font, 3, color, 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 608, 608)\n",
      "[[18, 461, 832, 870], [23, 489, 915, 807], [822, 638, 259, 480], [13, 529, 741, 854], [16, 513, 842, 895], [16, 504, 926, 930], [38, 522, 990, 885], [503, 667, 346, 601], [530, 671, 329, 598], [819, 662, 263, 587], [872, 640, 215, 638], [10, 568, 863, 853], [5, 564, 952, 863], [39, 637, 983, 725], [503, 719, 341, 558], [524, 691, 343, 604], [820, 655, 265, 755], [878, 657, 199, 758], [818, 725, 270, 702], [919, 988, 158, 253], [890, 790, 192, 596]]\n"
     ]
    }
   ],
   "source": [
    "dect_person('busuksa1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dect_person(\u001b[39m'\u001b[39m\u001b[39mC:/Users/USER/Desktop/com_project/data/rawdata/E/[emergency]accident11_088E/[emergency]accident11_088E_640.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [11], line 8\u001b[0m, in \u001b[0;36mdect_person\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m img\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mimread(path)\n\u001b[0;32m      5\u001b[0m \u001b[39m## 1. 이미지를 불러와 배열 형태로 저장 => 배열구조는 (1080,1920,3)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# img=cv2.resize(img,(516,516),fx=0.4,fy=0.4)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# ## 1. 불러온 이미지 사이즈 변경 => 배열구조는 (516,516,3)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m height,width,channels\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39;49mshape\n\u001b[0;32m     10\u001b[0m \u001b[39m# 객체 탐지\u001b[39;00m\n\u001b[0;32m     11\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(img, \u001b[39m0.00395\u001b[39m, (\u001b[39m608\u001b[39m, \u001b[39m608\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39mTrue\u001b[39;00m, crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dect_person('C:/Users/USER/Desktop/com_project/data/rawdata/E/[emergency]accident11_088E/[emergency]accident11_088E_640.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이트에 있는 코드 복사\n",
    "# 출처 : https://bskyvision.com/entry/pythonopenpose-openpose-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%B4%EC%84%9C-pose-estimation-%ED%95%B4%EB%B3%B4%EA%B8%B0-window-10-%ED%99%98%EA%B2%BD\n",
    "import cv2\n",
    " \n",
    "# 관절 번호: 머리는 0, 목은 1 등등\n",
    "BODY_PARTS = {\"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "              \"Background\": 15}\n",
    " \n",
    "# 관절들을 선으로 이을 때 쌍이 되는 것들\n",
    "POSE_PAIRS = [[\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "              [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "              [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"]]\n",
    " \n",
    "# 훈련된 network 세팅\n",
    "protoFile = \"pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "weightsFile = \"pose_iter_160000.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    " \n",
    "# 테스트 이미지 읽기\n",
    "image = cv2.imread(\"test.JPG\")\n",
    " \n",
    "# 테스트 이미지에서 height, width, color 정보 파악\n",
    "imageHeight, imageWidth, imageColor = image.shape\n",
    " \n",
    "# 테스트 이미지를 network에 넣기 위해 전처리\n",
    "inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (imageWidth, imageHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    " \n",
    "# 테스트 이미지를 network에 넣어줌\n",
    "net.setInput(inpBlob)\n",
    " \n",
    "# 결과 받아오기\n",
    "output = net.forward()\n",
    " \n",
    "H = output.shape[2]\n",
    "W = output.shape[3]\n",
    " \n",
    "# 검출된 관절 포인트를 테스트 이미지에 그려주기\n",
    "points = []\n",
    "for i in range(0, 15):\n",
    "    # 해당 관절 신뢰도 얻기\n",
    "    probMap = output[0, i, :, :]\n",
    " \n",
    "    # global 최대값 찾기\n",
    "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    " \n",
    "    # 원래 이미지에 맞게 점 위치 변경\n",
    "    x = (imageWidth * point[0]) / W\n",
    "    y = (imageHeight * point[1]) / H\n",
    " \n",
    "    # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로\n",
    "    if prob > 0.1:\n",
    "        cv2.circle(image, (int(x), int(y)), 3, (0, 255, 255), thickness=-1,\n",
    "                   lineType=cv2.FILLED)  # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "        cv2.putText(image, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        points.append((int(x), int(y)))\n",
    "    else:\n",
    "        points.append(None)\n",
    " \n",
    "cv2.imshow(\"Output-Keypoints\", image)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# 관절들을 선으로 연결해주기\n",
    "for pair in POSE_PAIRS:\n",
    "    partA = pair[0]  # Head\n",
    "    partA = BODY_PARTS[partA]  # 0\n",
    "    partB = pair[1]  # Neck\n",
    "    partB = BODY_PARTS[partB]  # 1\n",
    " \n",
    "    # print(partA,\" 와 \", partB, \" 연결\\n\")\n",
    "    if points[partA] and points[partB]:\n",
    "        cv2.line(image, points[partA], points[partB], (255, 0, 0), 2)\n",
    " \n",
    "cv2.imshow(\"Output-Keypoints-with-Lines\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복사한 코드를 활용하여 버스 승하차 이미지로 관절 탐지\n",
    "import cv2\n",
    " \n",
    "# 관절 번호: 머리는 0, 목은 1 등등\n",
    "BODY_PARTS = {\"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "              \"Background\": 15}\n",
    " \n",
    "# 관절들을 선으로 이을 때 쌍이 되는 것들\n",
    "POSE_PAIRS = [[\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "              [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "              [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"]]\n",
    " \n",
    "# 훈련된 network 세팅\n",
    "protoFile = \"pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "weightsFile = \"pose_iter_160000.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    " \n",
    "# 테스트 이미지 읽기\n",
    "image = cv2.imread(\"C:/Users/USER/Desktop/com_project/data/rawdata/E/[emergency]accident12_055E/[emergency]accident12_055E_760.jpg\")\n",
    " \n",
    "# 테스트 이미지에서 height, width, color 정보 파악\n",
    "imageHeight, imageWidth, imageColor = image.shape\n",
    " \n",
    "# 테스트 이미지를 network에 넣기 위해 전처리\n",
    "inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (imageWidth, imageHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    " \n",
    "# 테스트 이미지를 network에 넣어줌\n",
    "net.setInput(inpBlob)\n",
    " \n",
    "# 결과 받아오기\n",
    "output = net.forward()\n",
    " \n",
    "H = output.shape[2]\n",
    "W = output.shape[3]\n",
    " \n",
    "# 검출된 관절 포인트를 테스트 이미지에 그려주기\n",
    "points = []\n",
    "for i in range(0, 15):\n",
    "    # 해당 관절 신뢰도 얻기\n",
    "    probMap = output[0, i, :, :]\n",
    " \n",
    "    # global 최대값 찾기\n",
    "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    " \n",
    "    # 원래 이미지에 맞게 점 위치 변경\n",
    "    x = (imageWidth * point[0]) / W\n",
    "    y = (imageHeight * point[1]) / H\n",
    " \n",
    "    # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로\n",
    "    if prob > 0.1:\n",
    "        cv2.circle(image, (int(x), int(y)), 3, (0, 255, 255), thickness=-1,\n",
    "                   lineType=cv2.FILLED)  # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "        cv2.putText(image, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        points.append((int(x), int(y)))\n",
    "    else:\n",
    "        points.append(None)\n",
    " \n",
    "cv2.imshow(\"Output-Keypoints\", image)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# 관절들을 선으로 연결해주기\n",
    "for pair in POSE_PAIRS:\n",
    "    partA = pair[0]  # Head\n",
    "    partA = BODY_PARTS[partA]  # 0\n",
    "    partB = pair[1]  # Neck\n",
    "    partB = BODY_PARTS[partB]  # 1\n",
    " \n",
    "    # print(partA,\" 와 \", partB, \" 연결\\n\")\n",
    "    if points[partA] and points[partB]:\n",
    "        cv2.line(image, points[partA], points[partB], (255, 0, 0), 2)\n",
    " \n",
    "cv2.imshow(\"Output-Keypoints-with-Lines\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# MPII에서 각 파트 번호, 선으로 연결될 POSE_PAIRS\n",
    "BODY_PARTS = { \"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "                \"Background\": 15 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "                [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "                [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "                [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"] ]\n",
    "    \n",
    "# 각 파일 path\n",
    "# BASE_DIR=Path(__file__).resolve().parent\n",
    "protoFile = \"pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "weightsFile = \"pose_iter_160000.caffemodel\"\n",
    " \n",
    "# 위의 path에 있는 network 모델 불러오기\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "#쿠다 사용 안하면 밑에 이미지 크기를 줄이는게 나을 것이다\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA) #벡엔드로 쿠다를 사용하여 속도향상을 꾀한다\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA) # 쿠다 디바이스에 계산 요청\n",
    "\n",
    "\n",
    "###카메라랑 연결...?\n",
    "capture = cv2.VideoCapture(0) #카메라 정보 받아옴\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640) #카메라 속성 설정\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # width:너비, height: 높이\n",
    "\n",
    "inputWidth=320;\n",
    "inputHeight=240;\n",
    "inputScale=1.0/255;\n",
    "\n",
    " \n",
    "#반복문을 통해 카메라에서 프레임을 지속적으로 받아옴\n",
    "while cv2.waitKey(1) <0:  #아무 키나 누르면 끝난다.\n",
    "    #웹캠으로부터 영상 가져옴\n",
    "    hasFrame, frame = capture.read()  \n",
    "    \n",
    "    #영상이 커서 느리면 사이즈를 줄이자\n",
    "    #frame=cv2.resize(frame,dsize=(320,240),interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    #웹캠으로부터 영상을 가져올 수 없으면 웹캠 중지\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    \n",
    "    # \n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    \n",
    "    inpBlob = cv2.dnn.blobFromImage(frame, inputScale, (inputWidth, inputHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    "    \n",
    "    imgb=cv2.dnn.imagesFromBlob(inpBlob)\n",
    "    #cv2.imshow(\"motion\",(imgb[0]*255.0).astype(np.uint8))\n",
    "    \n",
    "    # network에 넣어주기\n",
    "    net.setInput(inpBlob)\n",
    "\n",
    "    # 결과 받아오기\n",
    "    output = net.forward()\n",
    "\n",
    "\n",
    "    # 키포인트 검출시 이미지에 그려줌\n",
    "    points = []\n",
    "    for i in range(0,15):\n",
    "        # 해당 신체부위 신뢰도 얻음.\n",
    "        probMap = output[0, i, :, :]\n",
    "    \n",
    "        # global 최대값 찾기\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "        # 원래 이미지에 맞게 점 위치 변경\n",
    "        x = (frameWidth * point[0]) / output.shape[3]\n",
    "        y = (frameHeight * point[1]) / output.shape[2]\n",
    "\n",
    "        # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로    \n",
    "        if prob > 0.1 :    \n",
    "            cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 255), thickness=-1, lineType=cv2.FILLED) # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "            cv2.putText(frame, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
    "            points.append((int(x), int(y)))\n",
    "        else :\n",
    "            points.append(None)\n",
    "    \n",
    "    \n",
    "\n",
    "    # 각 POSE_PAIRS별로 선 그어줌 (머리 - 목, 목 - 왼쪽어깨, ...)\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]             # Head\n",
    "        partA = BODY_PARTS[partA]   # 0\n",
    "        partB = pair[1]             # Neck\n",
    "        partB = BODY_PARTS[partB]   # 1\n",
    "        \n",
    "        #partA와 partB 사이에 선을 그어줌 (cv2.line)\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(frame, points[partA], points[partB], (0, 255, 0), 2)\n",
    "    \n",
    "            \n",
    "    cv2.imshow(\"Output-Keypoints\",frame)\n",
    " \n",
    "capture.release()  #카메라 장치에서 받아온 메모리 해제\n",
    "cv2.destroyAllWindows() #모든 윈도우 창 닫음}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 person: 96.94\n",
      "1 person: 94.72\n",
      "7 person: 85.96\n",
      "8 person: 98.58\n",
      "10 person: 95.88\n",
      "11 person: 84.05\n",
      "13 person: 92.59\n",
      "=== A frame took 0.411 seconds\n",
      "0 person: 96.51\n",
      "1 person: 96.11\n",
      "9 person: 83.26\n",
      "10 person: 99.01\n",
      "12 person: 94.15\n",
      "13 person: 72.81\n",
      "15 person: 92.70\n",
      "=== A frame took 0.350 seconds\n",
      "0 person: 93.48\n",
      "1 person: 92.85\n",
      "7 person: 57.92\n",
      "8 person: 80.19\n",
      "9 person: 99.03\n",
      "11 person: 92.32\n",
      "12 person: 73.26\n",
      "14 person: 97.35\n",
      "=== A frame took 0.389 seconds\n",
      "0 person: 95.44\n",
      "1 person: 94.66\n",
      "9 person: 74.03\n",
      "10 person: 74.73\n",
      "12 person: 99.43\n",
      "14 person: 93.84\n",
      "15 person: 80.44\n",
      "17 person: 98.81\n",
      "=== A frame took 0.326 seconds\n",
      "0 person: 94.05\n",
      "1 person: 92.36\n",
      "8 person: 75.09\n",
      "10 person: 99.27\n",
      "12 person: 91.96\n",
      "13 person: 73.37\n",
      "16 person: 59.18\n",
      "17 person: 98.84\n",
      "=== A frame took 0.303 seconds\n",
      "1 person: 91.63\n",
      "6 person: 95.20\n",
      "7 person: 50.71\n",
      "8 person: 73.68\n",
      "10 person: 99.00\n",
      "11 person: 90.98\n",
      "12 person: 68.46\n",
      "15 person: 98.67\n",
      "=== A frame took 0.341 seconds\n",
      "1 person: 85.00\n",
      "10 person: 95.37\n",
      "11 person: 65.83\n",
      "13 person: 99.04\n",
      "14 person: 86.45\n",
      "15 person: 56.80\n",
      "18 person: 98.06\n",
      "=== A frame took 0.325 seconds\n",
      "1 person: 85.39\n",
      "3 person: 90.42\n",
      "8 person: 69.06\n",
      "11 person: 60.01\n",
      "13 person: 98.70\n",
      "16 person: 98.76\n",
      "=== A frame took 0.321 seconds\n",
      "1 person: 91.13\n",
      "2 person: 80.00\n",
      "5 person: 68.46\n",
      "11 person: 62.03\n",
      "14 person: 98.93\n",
      "15 person: 73.29\n",
      "16 person: 71.23\n",
      "18 person: 53.58\n",
      "19 person: 98.54\n",
      "=== A frame took 0.328 seconds\n",
      "1 person: 84.69\n",
      "2 person: 90.76\n",
      "8 person: 71.18\n",
      "10 person: 98.75\n",
      "11 person: 53.02\n",
      "12 person: 98.05\n",
      "=== A frame took 0.328 seconds\n",
      "2 person: 92.01\n",
      "4 person: 91.23\n",
      "8 person: 78.46\n",
      "10 person: 98.76\n",
      "11 person: 71.70\n",
      "12 person: 98.00\n",
      "=== A frame took 0.311 seconds\n",
      "2 person: 95.85\n",
      "6 person: 94.54\n",
      "7 person: 58.10\n",
      "8 person: 70.26\n",
      "11 person: 77.83\n",
      "13 person: 98.59\n",
      "15 person: 98.40\n",
      "=== A frame took 0.316 seconds\n",
      "2 person: 92.06\n",
      "6 person: 96.71\n",
      "7 person: 67.39\n",
      "9 person: 89.06\n",
      "11 person: 98.45\n",
      "13 person: 95.98\n",
      "=== A frame took 0.323 seconds\n",
      "8 person: 93.62\n",
      "9 person: 88.84\n",
      "10 person: 97.06\n",
      "13 person: 98.50\n",
      "18 person: 97.75\n",
      "=== A frame took 0.330 seconds\n",
      "2 person: 89.29\n",
      "7 person: 94.45\n",
      "9 person: 98.57\n",
      "12 person: 98.51\n",
      "17 person: 97.16\n",
      "19 person: 81.96\n",
      "=== A frame took 0.331 seconds\n",
      "2 person: 89.69\n",
      "5 person: 96.50\n",
      "10 person: 97.46\n",
      "12 person: 98.95\n",
      "17 person: 97.00\n",
      "18 person: 63.17\n",
      "=== A frame took 0.356 seconds\n",
      "2 person: 92.89\n",
      "4 person: 97.06\n",
      "8 person: 99.21\n",
      "11 person: 98.89\n",
      "15 person: 97.50\n",
      "16 person: 85.96\n",
      "=== A frame took 0.325 seconds\n",
      "1 person: 95.67\n",
      "2 person: 95.26\n",
      "11 person: 99.61\n",
      "13 person: 98.60\n",
      "15 person: 96.89\n",
      "17 person: 88.03\n",
      "=== A frame took 0.382 seconds\n",
      "2 person: 94.31\n",
      "3 person: 91.53\n",
      "10 person: 99.41\n",
      "12 person: 97.92\n",
      "13 person: 96.78\n",
      "15 person: 87.66\n",
      "=== A frame took 0.328 seconds\n",
      "2 person: 92.21\n",
      "3 person: 88.90\n",
      "9 person: 99.35\n",
      "13 person: 97.25\n",
      "15 person: 98.10\n",
      "17 person: 86.97\n",
      "=== A frame took 0.356 seconds\n",
      "2 person: 87.54\n",
      "3 person: 90.83\n",
      "8 person: 98.88\n",
      "11 person: 97.89\n",
      "13 person: 98.18\n",
      "15 person: 74.53\n",
      "=== A frame took 0.370 seconds\n",
      "2 person: 93.68\n",
      "6 person: 90.61\n",
      "8 person: 99.30\n",
      "10 person: 96.25\n",
      "12 person: 98.41\n",
      "14 person: 89.42\n",
      "=== A frame took 0.355 seconds\n",
      "2 person: 94.11\n",
      "6 person: 86.11\n",
      "8 person: 99.11\n",
      "10 person: 96.77\n",
      "12 person: 98.39\n",
      "14 person: 86.06\n",
      "=== A frame took 0.332 seconds\n",
      "3 person: 89.06\n",
      "7 person: 82.02\n",
      "9 person: 57.64\n",
      "10 person: 99.29\n",
      "12 person: 96.32\n",
      "15 person: 98.79\n",
      "17 person: 84.14\n",
      "=== A frame took 0.335 seconds\n",
      "3 person: 84.64\n",
      "5 person: 98.61\n",
      "6 person: 88.91\n",
      "8 person: 62.49\n",
      "9 person: 99.22\n",
      "11 person: 98.79\n",
      "14 person: 82.26\n",
      "=== A frame took 0.344 seconds\n",
      "1 person: 86.38\n",
      "4 person: 97.69\n",
      "5 person: 73.24\n",
      "6 person: 52.96\n",
      "7 person: 98.57\n",
      "9 person: 97.49\n",
      "11 person: 83.06\n",
      "12 person: 53.26\n",
      "=== A frame took 0.354 seconds\n",
      "2 person: 90.99\n",
      "5 person: 97.60\n",
      "6 person: 69.37\n",
      "7 person: 98.58\n",
      "10 person: 51.27\n",
      "11 person: 94.70\n",
      "12 person: 83.31\n",
      "=== A frame took 0.356 seconds\n",
      "2 person: 88.42\n",
      "5 person: 97.46\n",
      "7 person: 80.30\n",
      "8 person: 98.85\n",
      "11 person: 70.43\n",
      "13 person: 94.69\n",
      "14 person: 88.11\n",
      "=== A frame took 0.355 seconds\n",
      "1 person: 86.24\n",
      "3 person: 96.72\n",
      "5 person: 91.33\n",
      "6 person: 96.34\n",
      "10 person: 73.54\n",
      "13 person: 93.03\n",
      "14 person: 90.44\n",
      "=== A frame took 0.406 seconds\n",
      "0 person: 87.98\n",
      "3 person: 97.50\n",
      "4 person: 66.31\n",
      "6 person: 92.36\n",
      "9 person: 70.33\n",
      "12 person: 63.45\n",
      "13 person: 89.66\n",
      "=== A frame took 0.349 seconds\n",
      "1 person: 87.90\n",
      "5 person: 96.59\n",
      "6 person: 92.66\n",
      "7 person: 70.38\n",
      "9 person: 64.03\n",
      "14 person: 88.21\n",
      "=== A frame took 0.371 seconds\n",
      "0 person: 87.03\n",
      "5 person: 94.94\n",
      "6 person: 70.95\n",
      "7 person: 87.75\n",
      "8 person: 62.33\n",
      "10 person: 85.97\n",
      "14 person: 87.68\n",
      "=== A frame took 0.426 seconds\n",
      "0 person: 91.59\n",
      "5 person: 93.96\n",
      "6 person: 75.11\n",
      "7 person: 54.44\n",
      "8 person: 86.43\n",
      "12 person: 76.56\n",
      "15 person: 86.36\n",
      "=== A frame took 0.374 seconds\n",
      "1 person: 87.34\n",
      "6 person: 58.25\n",
      "7 person: 81.40\n",
      "9 person: 91.06\n",
      "11 person: 86.75\n",
      "13 person: 91.26\n",
      "14 person: 77.38\n",
      "=== A frame took 0.357 seconds\n",
      "3 person: 88.75\n",
      "4 person: 79.87\n",
      "5 person: 66.38\n",
      "7 person: 80.96\n",
      "8 person: 88.20\n",
      "11 person: 95.67\n",
      "13 person: 70.17\n",
      "=== A frame took 0.385 seconds\n",
      "4 person: 90.08\n",
      "6 person: 81.87\n",
      "7 person: 74.59\n",
      "9 person: 69.81\n",
      "10 person: 84.97\n",
      "13 person: 94.48\n",
      "16 person: 93.72\n",
      "=== A frame took 0.370 seconds\n",
      "2 person: 95.98\n",
      "3 person: 92.41\n",
      "6 person: 90.80\n",
      "9 person: 89.84\n",
      "11 person: 93.91\n",
      "12 person: 89.14\n",
      "14 person: 54.84\n",
      "15 person: 52.78\n",
      "16 person: 52.27\n",
      "=== A frame took 0.384 seconds\n",
      "1 person: 97.86\n",
      "4 person: 92.31\n",
      "5 person: 93.98\n",
      "7 person: 98.70\n",
      "8 person: 80.48\n",
      "13 person: 85.82\n",
      "15 person: 58.85\n",
      "=== A frame took 0.397 seconds\n",
      "1 person: 98.10\n",
      "7 person: 76.59\n",
      "9 person: 98.42\n",
      "12 person: 95.01\n",
      "13 person: 95.42\n",
      "15 person: 92.50\n",
      "16 person: 72.96\n",
      "=== A frame took 0.363 seconds\n",
      "2 person: 96.53\n",
      "6 person: 99.64\n",
      "9 person: 95.01\n",
      "10 person: 93.59\n",
      "14 person: 86.92\n",
      "16 person: 87.78\n",
      "17 person: 63.97\n",
      "18 person: 70.80\n",
      "=== A frame took 0.362 seconds\n",
      "2 person: 98.80\n",
      "5 person: 99.12\n",
      "7 person: 97.30\n",
      "9 person: 90.64\n",
      "11 person: 92.77\n",
      "13 person: 62.20\n",
      "15 person: 53.05\n",
      "=== A frame took 0.373 seconds\n",
      "3 person: 99.50\n",
      "6 person: 84.82\n",
      "10 person: 99.47\n",
      "11 person: 98.98\n",
      "12 person: 61.41\n",
      "16 person: 88.42\n",
      "18 person: 51.84\n",
      "=== A frame took 0.386 seconds\n",
      "1 person: 99.51\n",
      "6 person: 86.44\n",
      "8 person: 83.36\n",
      "10 person: 97.96\n",
      "11 person: 98.74\n",
      "12 person: 64.37\n",
      "16 person: 54.63\n",
      "=== A frame took 0.396 seconds\n",
      "1 person: 99.72\n",
      "3 person: 63.69\n",
      "7 person: 61.79\n",
      "9 person: 86.57\n",
      "11 person: 81.09\n",
      "12 person: 98.32\n",
      "14 person: 76.44\n",
      "17 person: 51.78\n",
      "18 person: 52.17\n",
      "=== A frame took 0.382 seconds\n",
      "1 person: 99.69\n",
      "3 person: 51.48\n",
      "4 person: 56.25\n",
      "8 person: 64.76\n",
      "9 person: 88.19\n",
      "10 person: 90.86\n",
      "12 person: 98.21\n",
      "14 person: 83.26\n",
      "17 person: 71.67\n",
      "18 person: 53.63\n",
      "19 person: 53.82\n",
      "20 person: 51.69\n",
      "=== A frame took 0.427 seconds\n",
      "3 person: 99.71\n",
      "4 person: 53.35\n",
      "5 person: 58.59\n",
      "8 person: 84.57\n",
      "9 person: 99.19\n",
      "11 person: 80.78\n",
      "14 person: 62.97\n",
      "15 person: 64.03\n",
      "=== A frame took 0.388 seconds\n",
      "3 person: 99.46\n",
      "4 person: 55.50\n",
      "7 person: 78.12\n",
      "8 person: 99.52\n",
      "9 person: 82.78\n",
      "15 person: 59.84\n",
      "=== A frame took 0.406 seconds\n",
      "2 person: 98.63\n",
      "4 person: 56.82\n",
      "6 person: 83.46\n",
      "7 person: 99.62\n",
      "9 person: 85.96\n",
      "=== A frame took 0.422 seconds\n",
      "1 person: 99.17\n",
      "3 person: 55.53\n",
      "4 person: 78.02\n",
      "5 person: 99.30\n",
      "7 person: 73.99\n",
      "13 person: 50.92\n",
      "14 person: 59.54\n",
      "=== A frame took 0.374 seconds\n",
      "0 person: 98.83\n",
      "2 person: 53.60\n",
      "6 person: 99.53\n",
      "8 person: 82.70\n",
      "12 person: 81.27\n",
      "13 person: 54.98\n",
      "14 person: 64.66\n",
      "=== A frame took 0.373 seconds\n",
      "0 person: 98.78\n",
      "5 person: 98.80\n",
      "6 person: 79.40\n",
      "13 person: 82.71\n",
      "15 person: 53.16\n",
      "16 person: 69.47\n",
      "=== A frame took 0.375 seconds\n",
      "0 person: 99.01\n",
      "4 person: 53.97\n",
      "7 person: 88.72\n",
      "9 person: 99.52\n",
      "14 person: 87.07\n",
      "15 person: 65.47\n",
      "16 person: 75.91\n",
      "=== A frame took 0.397 seconds\n",
      "0 person: 99.18\n",
      "4 person: 52.05\n",
      "7 person: 93.18\n",
      "9 person: 99.28\n",
      "11 person: 90.54\n",
      "15 person: 66.28\n",
      "16 person: 74.77\n",
      "=== A frame took 0.421 seconds\n",
      "--(!) No captured frame -- Break!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time # -- 프레임 계산을 위해 사용\n",
    "\n",
    "\n",
    "vedio_path = 'test.mp4' #-- 사용할 영상 경로\n",
    "min_confidence = 0.5\n",
    "\n",
    "def detectAndDisplay(frame):\n",
    "    start_time = time.time()\n",
    "    img = cv2.resize(frame, None, fx=0.8, fy=0.8)\n",
    "    height, width, channels = img.shape\n",
    "    #cv2.imshow(\"Original Image\", img)\n",
    "\n",
    "    #-- 창 크기 설정\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    #-- 탐지한 객체의 클래스 예측 \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            #-- 원하는 class id 입력 / coco.names의 id에서 -1 할 것 \n",
    "            if class_id == 0 and confidence > min_confidence:\n",
    "                #-- 탐지한 객체 박싱\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "               \n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = \"{}: {:.2f}\".format(classes[class_ids[i]], confidences[i]*100)\n",
    "            print(i, label)\n",
    "            color = colors[i] #-- 경계 상자 컬러 설정 / 단일 생상 사용시 (255,255,255)사용(B,G,R)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label, (x, y - 5), font, 1, color, 1)\n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(\"=== A frame took {:.3f} seconds\".format(process_time))\n",
    "    cv2.imshow(\"YOLO test\", img)\n",
    "    \n",
    "#-- yolo 포맷 및 클래스명 불러오기\n",
    "model_file = './yolov3.weights' #-- 본인 개발 환경에 맞게 변경할 것\n",
    "config_file = './yolov3.cfg' #-- 본인 개발 환경에 맞게 변경할 것\n",
    "net = cv2.dnn.readNet(model_file, config_file)\n",
    "\n",
    "#-- GPU 사용\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "#-- 클래스(names파일) 오픈 / 본인 개발 환경에 맞게 변경할 것\n",
    "classes = []\n",
    "with open(\"./coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "#-- 비디오 활성화\n",
    "cap = cv2.VideoCapture(vedio_path) #-- 웹캠 사용시 vedio_path를 0 으로 변경\n",
    "if not cap.isOpened:\n",
    "    print('--(!)Error opening video capture')\n",
    "    exit(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        print('--(!) No captured frame -- Break!')\n",
    "        break\n",
    "    detectAndDisplay(frame)\n",
    "    #-- q 입력시 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PY_39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f383b27726cf6678b0d04baf9693a2bc69529457fb17a6accda21f03ae2e2b23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
